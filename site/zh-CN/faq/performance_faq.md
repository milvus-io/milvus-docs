---
id: performance_faq.md
---

# 性能优化问题

<!-- TOC -->

- [为什么重启 Milvus 服务端之后，第一次搜索时间非常长？](#为什么重启-milvus-服务端之后第一次搜索时间非常长)            
- [为什么搜索的速度非常慢？](#为什么搜索的速度非常慢)            
- [如何进行性能调优？](#如何进行性能调优)           
- [建立索引时需要设置 `nlist` 值，如何选择该值大小？](#建立索引时需要设置-nlist-值如何选择该值大小)            
- [为什么有时候小的数据集查询时间反而更长？](#为什么有时候小的数据集查询时间反而更长)             
- [为什么查询时 GPU 一直空闲？](#为什么查询时-gpu-一直空闲)            
- [为什么数据插入后不能马上被搜索到？](#为什么数据插入后不能马上被搜索到)          
- [数据量不大，查询批量很小时，为什么会出现 CPU 用量瓶颈，负载用不满的情况？](#数据量不大查询批量很小时为什么会出现-cpu-用量瓶颈负载用不满的情况)         
- [创建集合时 `index_file_size` 如何设置能达到性能最优？](#创建集合时-index_file_size-如何设置能达到性能最优)            
- [Milvus 的导入性能如何？](#milvus-的导入性能如何)             
- [边插入边搜索会影响搜索速度吗？](#边插入边搜索会影响搜索速度吗)             
- [批量搜索时，用多线程的收益大吗？](#批量搜索时用多线程的收益大吗)             
- [为什么同样的数据量，用 GPU 查询比 CPU 查询慢？](#为什么同样的数据量用-gpu-查询比-cpu-查询慢)
- [仍有问题没有得到解答？](#仍有问题没有得到解答)


<!-- /TOC -->


#### 为什么重启 Milvus 服务端之后，第一次搜索时间非常长？

重启后第一次搜索时，会将数据从磁盘加载到内存，所以这个时间会比较长。可以在 **server_config.yaml** 中开启 `preload_table`，在内存允许的情况下尽可能多地加载集合。这样在每次重启服务端之后，数据都会先载入到内存中，可以解决第一次搜索耗时很长的问题。或者在查询前，调用方法 `load_collection(collection_name)` 将该集合加载到内存。


#### 为什么搜索的速度非常慢？

请首先检查 **server_config.yaml** 的 `cache.cache_size` 参数是否大于集合中的数据量。

#### 如何进行性能调优？

- 确保配置文件中的参数 `cpu_cache_capacity` 值大于集合中的数据量。
- 调整 `use_blas_threhold` 的值（检查 `nq` 是否小于 `use_blas_threhold`）。
- 确保所有数据文件都建立了索引。
- 检查服务器上是否有其他进程在占用 CPU 资源。
- 调整参数 `index_file_size` 和 `nlist` 的值。
- 如果检索性能不稳定，可在启动 Milvus 时添加参数 `-e OMP_NUM_THREADS=NUM`，其中 `NUM` 为 CPU 逻辑核数的 2/3。

详见 [性能调优](tuning.md)。

#### 建立索引时需要设置 `nlist` 值，如何选择该值大小？

该值需要根据具体的使用情况去设置。详情参阅文章 [如何设置 Milvus 参数](https://www.milvus.io/cn/blogs/2020-2-16-api-setting.md) 和文档 [性能调优 > 索引](tuning.md#索引)。

#### 为什么有时候小的数据集查询时间反而更长？

如果数据文件的大小小于创建集合时 `index_file_size` 参数的值，Milvus 则不会为此数据文件构建索引。因此，小的数据集有可能查询时间会更长。此时可以再次调用 `create_index` 建立索引。


#### 为什么查询时 GPU 一直空闲？

此时应该是在用 CPU 进行查询。如果要用 GPU 进行查询，需要在配置文件中将 `gpu_search_threshold` 的值设置为大于 `nq` (每次批量查询的向量数) 。可以将 `gpu_search_threshold` 的值调整为期望开启 GPU 搜索的批量查询的 `nq` 数。若 `nq` 小于该值，则用 CPU 查询，否则使用 GPU 查询。不建议在查询批量较小时使用 GPU 搜索。

#### 为什么数据插入后不能马上被搜索到？

因为数据还没有落盘。要确保数据插入后立刻能搜索到，可以手动调用 `flush` 接口。但是频繁调用 `flush` 接口可能会产生大量小数据文件，从而导致查询变慢。


#### 数据量不大，查询批量很小时，为什么会出现 CPU 用量瓶颈，负载用不满的情况？

`nq` = 100 以下，且数据量也不大的时候确实会出现这个现象。Milvus 在计算时，批量内的查询是并行处理的，如果批量不大且数据量也不大的话，并行度不高，CPU 利用率也就不高了。

#### 创建集合时 `index_file_size` 如何设置能达到性能最优？

使用客户端创建集合时有一个 `index_file_size` 参数，用来指定数据存储时单个文件的大小，其单位为 MB，默认值为 1024。当向量数据不断导入时，Milvus 会把数据增量式地合并成文件。当某个文件达到 `index_file_size` 所设置的值之后，这个文件就不再接受新的数据，Milvus 会把新的数据存成另外一个文件。这些都是原始向量数据文件，如果建立了索引，则每个原始文件会对应生成一个索引文件。Milvus 在进行搜索时，是依次对每个索引文件进行搜索。

根据我们的经验，当 `index_file_size` 从 1024 改为 2048 时，搜索性能会有 30% ~ 50% 左右的提升。但要注意如果该值设得过大，有可能导致大文件无法加载进显存（甚至内存）。比如显存只有 2 GB，该参数设为 3 GB，显存明显放不下。

常用的 `index_file_size` 为 1024 MB 和 2048 MB。如果后续会持续地向集合中导入增量数据，为了避免查询时未建立索引的数据文件过大，建议这种情况下将该值设置为 256 MB 或者 512 MB。

#### Milvus 的导入性能如何？

客户端和服务端在同一台物理机上时，10 万条 128 维的向量导入需要约 0.8 秒（基于 SSD 磁盘）。这个具体也要看磁盘的 I/O 速度。

#### 边插入边搜索会影响搜索速度吗？

插入时，后台会定时落盘，然后把小文件合并到大文件里去。这个动作会消耗内存和 I/O, 但 CPU 的消耗不大，所以资源这块对搜索影响不大。

但是新增数据插入后，未建立索引的数据会增多，对这部分未建立索引的数据的搜索会比较慢。另外，新插入的数据在第一次被搜索时需要从磁盘加载到内存，这里也会有极短的耗时。

如果文件合并的大小达到 `index_file_size`，就会触发后台的建索引动作。在 0.9.0 以后，搜索请求会打断建索引任务，这个过程会有一秒左右的延时。

#### 批量搜索时，用多线程的收益大吗？

多线程查询，如果是小批量（`nq` < 200）的话，后台会合并查询请求。虽然单个的耗时不变，但 QPS 会高很多。如果是大批量查询的话，就不会有什么优势。

#### 为什么同样的数据量，用 GPU 查询比 CPU 查询慢？

查询时，如果 `nq`（每次查询的向量条数）较小时，用 CPU 查询会更快。只有 `nq` 较大（约大于 500）时，用 GPU 才会有优势。因为用 GPU 查询时，存在将数据从内存加载到显存这个过程。只有在 `nq` 足够大时，用 GPU 查询节省的计算时间才能抵消掉数据加载这一过程的时间，从而体现出 GPU 查询的优势。在 Milvus 中，数据不会常驻显存，每次用 GPU 查询，都需要将数据从内存加载到显存。





#### 仍有问题没有得到解答？

如果仍有其他问题，你可以：

- 在 GitHub 上访问 [Milvus](https://github.com/milvus-io/milvus/issues)，提问、分享、交流，帮助其他用户。
- 加入我们的 [Slack 社区](https://join.slack.com/t/milvusio/shared_invite/enQtNzY1OTQ0NDI3NjMzLWNmYmM1NmNjOTQ5MGI5NDhhYmRhMGU5M2NhNzhhMDMzY2MzNDdlYjM5ODQ5MmE3ODFlYzU3YjJkNmVlNDQ2ZTk)，与其他用户讨论交流。