---
id: architecture_overview.md
title: 系统架构
---

# Milvus 系统架构

Milvus 支持两种部署模式，单机模式（standalone）和分布式模式（cluster）。两种模式具备完全相同的能力，用户可以根据数据规模、访问量等因素选择适合自己的模式。Standalone 模式部署的 Milvus 暂时不支持在线升级为 cluster 模式。

## 单机架构

![Standalone_architecture](../../../assets/standalone_architecture.jpeg)

单机版 Milvus 包括三个组件：
- Milvus 负责提供系统的核心功能。
- Etcd 是元数据引擎，用于管理 Milvus 内部组件的元数据访问和存储，例如 proxy node、index node 等。 
- MinIO 是存储引擎，负责维护 Milvus 内部组件的数据持久化，例如 proxy node、index node 等。

## 分布式架构

![Distributed_architecture](../../../assets/distributed_architecture.jpeg)

分布式版 Milvus 由八个微服务组件和三个第三方底层服务组件组成，每个微服务组件可使用 Kubernetes 独立部署。

**微服务组件：**
- Root coord
- Proxy 
- Query coord 
- Query node 
- Index coord 
- Index node 
- Data coord 
- Data node

**第三方底层服务组件：**

- Etcd 负责存储集群中各组件的元数据信息。
- MinIO 负责处理集群中大型文件的数据持久化，如索引文件和全二进制日志文件。
- Pulsar 负责管理近期 collection 更新操作的日志，并提供流式日志输出和日志订阅服务。

## 相比传统设计的优势

Milvus 2.0 面向云原生设计，相比传统设计具备以下优势：
- 纯分布式设计，水平扩展。
- 微服务化设计，解耦合便于应对多变的应用负载，解决隔离性和相互影响。
- 存储计算分离，所有计算节点无状态，提升弹性和可用性。
- 流批一体，兼顾数据的实时性和结果的一致性。

整个 Milvus 围绕日志为核心来设计，遵循日志即数据的准则，因此在 2.0 版本中没有维护物理上的表，而是通过日志持久化和日志快照来保证数据的可靠性。

![Log_mechanism](../../../assets/log_mechanism.png)

日志系统作为系统的主干，承担了数据持久化和解耦的作用。通过日志的发布—订阅机制，Milvus 将系统的读、写组件解耦。一个极致简化的模型如上图所示，整个系统主要由两个角⾊构成，分别是“日志序列”与“⽇志订阅者”。其中的“⽇志序列”记录了所有改变库表状态的操作。“日志订阅者”通过订阅日志序列更新本地数据，以只读副本的⽅式提供服务。 发布—订阅机制的出现也给系统预留了很大的拓展空间，便于 change data capture（CDC）、全球部署等功能的拓展。 

![Architecture_diagram](../../../assets/architecture_diagram.png)

从架构上来看，Milvus 遵循数据流和控制流分离，整体分为了四个层次，分别为接入层（access layer）、协调服务（coordinator service）、执行层（worker node）和存储层（storage）。各个层次相互独立，独立扩展和容灾。

#### 接入层

接入层对外暴露了客户端连接的 endpoint，负责处理客户端链接，进行用户请求的静态验证和基本的动态检查，完成向其他系统组件内组合调用获取操作结果并向客户端返回。Proxy 本身是无状态的，一般通过负载均衡组件（Nginx、Kubernetes Ingress、NodePort、LVS）对外提供统一的访问地址并提供服务。Milvus 是典型的大规模并行处理（MPP）架构，proxy 也会负责从执行节点返回的结果，进行全局聚合和后处理后返回。

#### 协调服务

协调服务是系统的大脑，负责了集群拓扑节点管理、负载均衡、时间戳生成、数据声明和数据管理等功能，通过依赖元数据存储实现元信息持久化和故障切换。协调服务共计四种角色：

- **Root coordinator (root coord)** 负责处理 DDL、DCL 请求，比如创建删除 collection、partition、index等，同时负责维护中心授时服务 TSO 和时间窗口的推进。
- **Query coordinator (query coord)** 负责管理 query node 的拓扑结构、query node 之间数据段的负载均衡以及 growing segment 到 sealed segment的切换流程（handoff）。
- **Data coordinator (data coord)** 负责管理 data node 的拓扑结构，维护数据的元信息，触发 segment 的 flush、compaction等后台数据操作。
- **Index coordinator (index coord)** 负责管理 index node 的拓扑结构，管理索引构建任务，维护索引元信息。

#### 执行节点

执行节点是系统的四肢，负责完成协调服务下发的指令和 proxy 发起的 DML 命令。由于采取了存储计算分离，执行节点是无状态的，可以配合 Kubernetes 快速实现扩缩容和故障恢复。Worker node分为三种角色：
- **Query node** 通过订阅消息存储获取增量日志数据，基于对象存储获取历史数据，提供标量+向量的混合查询和搜索功能。
- **Data node** 通过订阅消息存储获取增量日志数据，处理写入、删除等请求，并将日志数据打包存储在对象存储上实现日志快照持久化。
- **Index node** 负责执行索引构建任务。Index node不需要常驻于内存，可以通过 serverless 的模式实现。

#### 存储服务

存储服务是系统的骨骼，负责 Milvus 数据的持久化，分为元数据存储（meta store）、消息存储（log broker）和对象存储（object storage）三个部分。

- **元数据存储：**负责存储元信息的快照，比如 collection schema 信息、节点状态信息、消息消费的 checkpoint 等。元信息存储需要极高的可用性、强一致和事务支持，因此 etcd 是这个场景下的不二选择。除此之外，etcd 还承担了服务注册和健康检查的职责。
- **消息存储：**一套支持回放的发布订阅系统，用于持久化流式写入的数据，以及可靠的异步执行查询，事件通知和结果返回。执行节点宕机恢复时，通过回放消息存储保证增量数据的完整性。目前分布式 Milvus 依赖 Pulsar 作为消息存储，Milvus standalone 依赖 RocksDB 作为消息存储。消息存储也可以替换为 Kafka、Pravega 等流式存储。
- **对象存储：**负责存储日志的快照文件、标量/向量索引文件以及查询的中间处理结果。AWS S3 和Azure Blob已经成为最广泛使用的低成本存储，具备云原生弹性、按需计费的优点。我们也支持MinIO，一个开源的轻量级对象存储服务。由于对象存储访问延迟较高，且需要按照查询计费，因此 Milvus 未来将会支持基于内存/SSD 的缓存池，通过冷热分离的方式提升性能降低成本。

## 关键路径

